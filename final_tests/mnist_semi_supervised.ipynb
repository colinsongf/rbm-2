{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from mnist_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_SAMPLES_PER_DIGIT = 60\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "BATCH_SIZE = 40\n",
    "NUM_BATCHES = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 1 Layers\\npretrain_path = 'fc_1L/test/rbm_pretrain.pkl'\\n[W_1, vb_1, hb_1] = cPickle.load(file(pretrain_path, 'rb'))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = 'mnist.pkl'\n",
    "_, (valid_imgs, valid_labels), (test_imgs, test_labels) = cPickle.load(file(data_path, 'rb'))\n",
    "train_imgs, train_labels = gen_small_mnist(600,data_path)\n",
    "\n",
    "# Load DBM pretrained data\n",
    "\n",
    "# 3 Layers\n",
    "pretrain_path = 'fc_3L/test/rbm_pretrain.pkl'\n",
    "[W_1, vb_1, hb_1, W_2, vb_2, hb_2, W_3, vb_3, hb_3] = cPickle.load(file(pretrain_path, 'rb'))\n",
    "'''\n",
    "# 1 Layers\n",
    "pretrain_path = 'fc_1L/test/rbm_pretrain.pkl'\n",
    "[W_1, vb_1, hb_1] = cPickle.load(file(pretrain_path, 'rb'))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defined function for calculating losses\n",
    "def vhv(images):\n",
    "    dbm_h1 = tf.nn.sigmoid(tf.matmul(images,W_1)+hb_1)\n",
    "    dbm_h2 = tf.nn.sigmoid(tf.matmul(dbm_h1,W_2)+hb_2)\n",
    "    dbm_h3 = tf.nn.sigmoid(tf.matmul(dbm_h2,W_3)+hb_3)\n",
    "    dbm_v3 = tf.nn.sigmoid(tf.matmul(dbm_h3,np.transpose(W_3)+vb_3))\n",
    "    dbm_v2 = tf.nn.sigmoid(tf.matmul(dbm_v3,np.transpose(W_2)+vb_2))\n",
    "    dbm_v1 = tf.nn.sigmoid(tf.matmul(dbm_v2,np.transpose(W_1)+vb_1))\n",
    "    return dbm_v1\n",
    "    \n",
    "def vhv_get_loss(images,y_):\n",
    "    dbm_output = vhv(images)\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [784, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.nn.softmax(tf.matmul(dbm_output, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "    \n",
    "                           \n",
    "# Build computation graph\n",
    "# Input: images (batch_size * 784)\n",
    "def get_loss_plain(images,y_):\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [784, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.nn.softmax(tf.matmul(images, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "\n",
    "def get_loss_1L(images,y_):\n",
    "    # Fixed dbm part - as constant\n",
    "    dbm_output = tf.nn.sigmoid(tf.matmul(images,W_1)+hb_1)\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [1000, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.nn.softmax(tf.matmul(dbm_output, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "\n",
    "def get_loss_2L(images,y_):\n",
    "    # Fixed dbm part - as constant\n",
    "    dbm_h1 = tf.nn.sigmoid(tf.matmul(images,W_1)+hb_1)\n",
    "    dbm_output = tf.nn.sigmoid(tf.matmul(dbm_h1,W_2)+hb_2)\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [500, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.nn.softmax(tf.matmul(dbm_output, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "\n",
    "def get_loss_3L(images,y_):\n",
    "    # Fixed dbm part - as constant\n",
    "    dbm_h1 = tf.nn.sigmoid(tf.matmul(images,W_1)+hb_1)\n",
    "    dbm_h2 = tf.nn.sigmoid(tf.matmul(dbm_h1,W_2)+hb_2)\n",
    "    dbm_output = tf.nn.sigmoid(tf.matmul(dbm_h2,W_3)+hb_3)\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [1000, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.nn.softmax(tf.matmul(dbm_output, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "\n",
    "def get_loss_dropout(images,y_,train=False):\n",
    "    # Fixed dbm part - as constant\n",
    "    dbm_h1 = tf.nn.sigmoid(tf.matmul(images,W_1)+hb_1)\n",
    "    dbm_h2 = tf.nn.sigmoid(tf.matmul(dbm_h1,W_2)+hb_2)\n",
    "    dbm_output = tf.nn.sigmoid(tf.matmul(dbm_h2,W_3)+hb_3)\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [1000, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        if train is not None:\n",
    "          dbm_output = tf.nn.dropout(dbm_output, 0.5)\n",
    "        \n",
    "        y = tf.nn.softmax(tf.matmul(dbm_output, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "\n",
    "def get_loss_last_two_layers(images,y_):\n",
    "    # Fixed dbm part - as constant\n",
    "    dbm_h1 = tf.nn.sigmoid(tf.matmul(images,W_1)+hb_1)\n",
    "    dbm_h2 = tf.nn.sigmoid(tf.matmul(dbm_h1,W_2)+hb_2)\n",
    "    dbm_h3 = tf.nn.sigmoid(tf.matmul(dbm_h2,W_3)+hb_3)\n",
    "    dbm_output = tf.concat(1, [dbm_h2, dbm_h3])\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [1500, 10],\n",
    "                                  initializer=tf.truncated_normal_initializer(0, 0.01))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape = [10],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.nn.softmax(tf.matmul(dbm_output, weights) + biases)\n",
    "        if y_ != None:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y,y_))\n",
    "        else:\n",
    "            cross_entropy = None\n",
    "        \n",
    "        # Check training accuracy\n",
    "        correct_count = evaluation(y,y_)\n",
    "        \n",
    "    return cross_entropy,correct_count\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result@Batch 50:\n",
      "Average Training Loss: 2.122103\n",
      "Average Training Accuracy: 0.551500\n",
      "Validation Loss: 1.851574\n",
      "Validation Accuracy: 0.788100\n",
      "Result@Batch 100:\n",
      "Average Training Loss: 1.775086\n",
      "Average Training Accuracy: 0.826500\n",
      "Validation Loss: 1.685846\n",
      "Validation Accuracy: 0.895300\n",
      "Result@Batch 150:\n",
      "Average Training Loss: 1.669995\n",
      "Average Training Accuracy: 0.891500\n",
      "Validation Loss: 1.635653\n",
      "Validation Accuracy: 0.907300\n",
      "Result@Batch 200:\n",
      "Average Training Loss: 1.631470\n",
      "Average Training Accuracy: 0.907000\n",
      "Validation Loss: 1.609655\n",
      "Validation Accuracy: 0.917300\n",
      "Result@Batch 250:\n",
      "Average Training Loss: 1.592457\n",
      "Average Training Accuracy: 0.933500\n",
      "Validation Loss: 1.590941\n",
      "Validation Accuracy: 0.926100\n",
      "Result@Batch 300:\n",
      "Average Training Loss: 1.584694\n",
      "Average Training Accuracy: 0.935500\n",
      "Validation Loss: 1.581855\n",
      "Validation Accuracy: 0.927800\n",
      "Result@Batch 350:\n",
      "Average Training Loss: 1.576952\n",
      "Average Training Accuracy: 0.929000\n",
      "Validation Loss: 1.574225\n",
      "Validation Accuracy: 0.929000\n",
      "Result@Batch 400:\n",
      "Average Training Loss: 1.577986\n",
      "Average Training Accuracy: 0.932000\n",
      "Validation Loss: 1.566682\n",
      "Validation Accuracy: 0.932400\n",
      "Result@Batch 450:\n",
      "Average Training Loss: 1.565476\n",
      "Average Training Accuracy: 0.937500\n",
      "Validation Loss: 1.562425\n",
      "Validation Accuracy: 0.932000\n",
      "Result@Batch 500:\n",
      "Average Training Loss: 1.571378\n",
      "Average Training Accuracy: 0.922500\n",
      "Validation Loss: 1.559413\n",
      "Validation Accuracy: 0.934300\n",
      "Result@Batch 550:\n",
      "Average Training Loss: 1.554313\n",
      "Average Training Accuracy: 0.942000\n",
      "Validation Loss: 1.553603\n",
      "Validation Accuracy: 0.936100\n",
      "Result@Batch 600:\n",
      "Average Training Loss: 1.551572\n",
      "Average Training Accuracy: 0.946000\n",
      "Validation Loss: 1.552044\n",
      "Validation Accuracy: 0.938000\n",
      "Result@Batch 650:\n",
      "Average Training Loss: 1.544644\n",
      "Average Training Accuracy: 0.947000\n",
      "Validation Loss: 1.549503\n",
      "Validation Accuracy: 0.939000\n",
      "Result@Batch 700:\n",
      "Average Training Loss: 1.542834\n",
      "Average Training Accuracy: 0.949000\n",
      "Validation Loss: 1.547066\n",
      "Validation Accuracy: 0.938000\n",
      "Result@Batch 750:\n",
      "Average Training Loss: 1.542744\n",
      "Average Training Accuracy: 0.942000\n",
      "Validation Loss: 1.546547\n",
      "Validation Accuracy: 0.938700\n",
      "Result@Batch 800:\n",
      "Average Training Loss: 1.547583\n",
      "Average Training Accuracy: 0.939000\n",
      "Validation Loss: 1.543846\n",
      "Validation Accuracy: 0.940300\n",
      "Result@Batch 850:\n",
      "Average Training Loss: 1.539581\n",
      "Average Training Accuracy: 0.947000\n",
      "Validation Loss: 1.541678\n",
      "Validation Accuracy: 0.941100\n",
      "Result@Batch 900:\n",
      "Average Training Loss: 1.541434\n",
      "Average Training Accuracy: 0.943000\n",
      "Validation Loss: 1.539652\n",
      "Validation Accuracy: 0.942800\n",
      "Result@Batch 950:\n",
      "Average Training Loss: 1.545633\n",
      "Average Training Accuracy: 0.942000\n",
      "Validation Loss: 1.539307\n",
      "Validation Accuracy: 0.943000\n",
      "Result@Batch 1000:\n",
      "Average Training Loss: 1.531120\n",
      "Average Training Accuracy: 0.952500\n",
      "Validation Loss: 1.536888\n",
      "Validation Accuracy: 0.944000\n",
      "Result@Batch 1050:\n",
      "Average Training Loss: 1.529858\n",
      "Average Training Accuracy: 0.953500\n",
      "Validation Loss: 1.535822\n",
      "Validation Accuracy: 0.944300\n",
      "Result@Batch 1100:\n",
      "Average Training Loss: 1.536832\n",
      "Average Training Accuracy: 0.946000\n",
      "Validation Loss: 1.535129\n",
      "Validation Accuracy: 0.944700\n",
      "Result@Batch 1150:\n",
      "Average Training Loss: 1.523491\n",
      "Average Training Accuracy: 0.962000\n",
      "Validation Loss: 1.535310\n",
      "Validation Accuracy: 0.943100\n",
      "Result@Batch 1200:\n",
      "Average Training Loss: 1.537359\n",
      "Average Training Accuracy: 0.943500\n",
      "Validation Loss: 1.533705\n",
      "Validation Accuracy: 0.944400\n",
      "Result@Batch 1250:\n",
      "Average Training Loss: 1.525696\n",
      "Average Training Accuracy: 0.954000\n",
      "Validation Loss: 1.533455\n",
      "Validation Accuracy: 0.945400\n",
      "Result@Batch 1300:\n",
      "Average Training Loss: 1.525515\n",
      "Average Training Accuracy: 0.955000\n",
      "Validation Loss: 1.531328\n",
      "Validation Accuracy: 0.947100\n",
      "Result@Batch 1350:\n",
      "Average Training Loss: 1.519743\n",
      "Average Training Accuracy: 0.963000\n",
      "Validation Loss: 1.530699\n",
      "Validation Accuracy: 0.947400\n",
      "Result@Batch 1400:\n",
      "Average Training Loss: 1.524570\n",
      "Average Training Accuracy: 0.959000\n",
      "Validation Loss: 1.530396\n",
      "Validation Accuracy: 0.945300\n",
      "Result@Batch 1450:\n",
      "Average Training Loss: 1.517873\n",
      "Average Training Accuracy: 0.959500\n",
      "Validation Loss: 1.530450\n",
      "Validation Accuracy: 0.946600\n",
      "Result@Batch 1500:\n",
      "Average Training Loss: 1.529464\n",
      "Average Training Accuracy: 0.949500\n",
      "Validation Loss: 1.528937\n",
      "Validation Accuracy: 0.945900\n",
      "Result@Batch 1550:\n",
      "Average Training Loss: 1.518233\n",
      "Average Training Accuracy: 0.963000\n",
      "Validation Loss: 1.529483\n",
      "Validation Accuracy: 0.944600\n",
      "Result@Batch 1600:\n",
      "Average Training Loss: 1.524354\n",
      "Average Training Accuracy: 0.957500\n",
      "Validation Loss: 1.527674\n",
      "Validation Accuracy: 0.948100\n",
      "Result@Batch 1650:\n",
      "Average Training Loss: 1.522114\n",
      "Average Training Accuracy: 0.954000\n",
      "Validation Loss: 1.527020\n",
      "Validation Accuracy: 0.947200\n",
      "Result@Batch 1700:\n",
      "Average Training Loss: 1.509735\n",
      "Average Training Accuracy: 0.970500\n",
      "Validation Loss: 1.526317\n",
      "Validation Accuracy: 0.948800\n",
      "Result@Batch 1750:\n",
      "Average Training Loss: 1.516522\n",
      "Average Training Accuracy: 0.961500\n",
      "Validation Loss: 1.525750\n",
      "Validation Accuracy: 0.947900\n",
      "Result@Batch 1800:\n",
      "Average Training Loss: 1.530480\n",
      "Average Training Accuracy: 0.947000\n",
      "Validation Loss: 1.526098\n",
      "Validation Accuracy: 0.948300\n",
      "Result@Batch 1850:\n",
      "Average Training Loss: 1.523076\n",
      "Average Training Accuracy: 0.952500\n",
      "Validation Loss: 1.527080\n",
      "Validation Accuracy: 0.946900\n",
      "Result@Batch 1900:\n",
      "Average Training Loss: 1.516227\n",
      "Average Training Accuracy: 0.958000\n",
      "Validation Loss: 1.525080\n",
      "Validation Accuracy: 0.948300\n",
      "Result@Batch 1950:\n",
      "Average Training Loss: 1.507404\n",
      "Average Training Accuracy: 0.973000\n",
      "Validation Loss: 1.524838\n",
      "Validation Accuracy: 0.948000\n",
      "Result@Batch 2000:\n",
      "Average Training Loss: 1.509737\n",
      "Average Training Accuracy: 0.966500\n",
      "Validation Loss: 1.525240\n",
      "Validation Accuracy: 0.949400\n",
      "Result@Batch 2050:\n",
      "Average Training Loss: 1.512864\n",
      "Average Training Accuracy: 0.965500\n",
      "Validation Loss: 1.524559\n",
      "Validation Accuracy: 0.949700\n",
      "Result@Batch 2100:\n",
      "Average Training Loss: 1.507700\n",
      "Average Training Accuracy: 0.965500\n",
      "Validation Loss: 1.524106\n",
      "Validation Accuracy: 0.947900\n",
      "Result@Batch 2150:\n",
      "Average Training Loss: 1.516289\n",
      "Average Training Accuracy: 0.959500\n",
      "Validation Loss: 1.526412\n",
      "Validation Accuracy: 0.945200\n",
      "Result@Batch 2200:\n",
      "Average Training Loss: 1.512609\n",
      "Average Training Accuracy: 0.965500\n",
      "Validation Loss: 1.522301\n",
      "Validation Accuracy: 0.950900\n",
      "Result@Batch 2250:\n",
      "Average Training Loss: 1.517692\n",
      "Average Training Accuracy: 0.956000\n",
      "Validation Loss: 1.522768\n",
      "Validation Accuracy: 0.949100\n",
      "Result@Batch 2300:\n",
      "Average Training Loss: 1.513525\n",
      "Average Training Accuracy: 0.962500\n",
      "Validation Loss: 1.521605\n",
      "Validation Accuracy: 0.949900\n",
      "Result@Batch 2350:\n",
      "Average Training Loss: 1.501267\n",
      "Average Training Accuracy: 0.974000\n",
      "Validation Loss: 1.521697\n",
      "Validation Accuracy: 0.949700\n",
      "Result@Batch 2400:\n",
      "Average Training Loss: 1.512431\n",
      "Average Training Accuracy: 0.962500\n",
      "Validation Loss: 1.522080\n",
      "Validation Accuracy: 0.948800\n",
      "Result@Batch 2450:\n",
      "Average Training Loss: 1.511580\n",
      "Average Training Accuracy: 0.961500\n",
      "Validation Loss: 1.520274\n",
      "Validation Accuracy: 0.951500\n",
      "Result@Batch 2500:\n",
      "Average Training Loss: 1.509805\n",
      "Average Training Accuracy: 0.961000\n",
      "Validation Loss: 1.520372\n",
      "Validation Accuracy: 0.950700\n",
      "Result@Batch 2550:\n",
      "Average Training Loss: 1.505949\n",
      "Average Training Accuracy: 0.967000\n",
      "Validation Loss: 1.520850\n",
      "Validation Accuracy: 0.948800\n",
      "Result@Batch 2600:\n",
      "Average Training Loss: 1.516547\n",
      "Average Training Accuracy: 0.957500\n",
      "Validation Loss: 1.521718\n",
      "Validation Accuracy: 0.947600\n",
      "Result@Batch 2650:\n",
      "Average Training Loss: 1.507916\n",
      "Average Training Accuracy: 0.966500\n",
      "Validation Loss: 1.520585\n",
      "Validation Accuracy: 0.949400\n",
      "Result@Batch 2700:\n",
      "Average Training Loss: 1.509077\n",
      "Average Training Accuracy: 0.968500\n",
      "Validation Loss: 1.520248\n",
      "Validation Accuracy: 0.950200\n",
      "Result@Batch 2750:\n",
      "Average Training Loss: 1.513000\n",
      "Average Training Accuracy: 0.960500\n",
      "Validation Loss: 1.521608\n",
      "Validation Accuracy: 0.949700\n",
      "Result@Batch 2800:\n",
      "Average Training Loss: 1.509646\n",
      "Average Training Accuracy: 0.964000\n",
      "Validation Loss: 1.520110\n",
      "Validation Accuracy: 0.951100\n",
      "Result@Batch 2850:\n",
      "Average Training Loss: 1.508882\n",
      "Average Training Accuracy: 0.962500\n",
      "Validation Loss: 1.520054\n",
      "Validation Accuracy: 0.950800\n",
      "Result@Batch 2900:\n",
      "Average Training Loss: 1.501925\n",
      "Average Training Accuracy: 0.972500\n",
      "Validation Loss: 1.520166\n",
      "Validation Accuracy: 0.950300\n",
      "Result@Batch 2950:\n",
      "Average Training Loss: 1.505165\n",
      "Average Training Accuracy: 0.966500\n",
      "Validation Loss: 1.518728\n",
      "Validation Accuracy: 0.951400\n",
      "Result@Batch 3000:\n",
      "Average Training Loss: 1.512585\n",
      "Average Training Accuracy: 0.959500\n",
      "Validation Loss: 1.519155\n",
      "Validation Accuracy: 0.951100\n",
      "Result@Batch 3050:\n",
      "Average Training Loss: 1.500503\n",
      "Average Training Accuracy: 0.973000\n",
      "Validation Loss: 1.518484\n",
      "Validation Accuracy: 0.950900\n",
      "Result@Batch 3100:\n",
      "Average Training Loss: 1.505518\n",
      "Average Training Accuracy: 0.967500\n",
      "Validation Loss: 1.518131\n",
      "Validation Accuracy: 0.952100\n",
      "Result@Batch 3150:\n",
      "Average Training Loss: 1.502964\n",
      "Average Training Accuracy: 0.969500\n",
      "Validation Loss: 1.518270\n",
      "Validation Accuracy: 0.951800\n",
      "Result@Batch 3200:\n",
      "Average Training Loss: 1.502372\n",
      "Average Training Accuracy: 0.972500\n",
      "Validation Loss: 1.517710\n",
      "Validation Accuracy: 0.953100\n",
      "Result@Batch 3250:\n",
      "Average Training Loss: 1.503864\n",
      "Average Training Accuracy: 0.968000\n",
      "Validation Loss: 1.516953\n",
      "Validation Accuracy: 0.953000\n",
      "Result@Batch 3300:\n",
      "Average Training Loss: 1.509217\n",
      "Average Training Accuracy: 0.963000\n",
      "Validation Loss: 1.519699\n",
      "Validation Accuracy: 0.949800\n",
      "Result@Batch 3350:\n",
      "Average Training Loss: 1.504262\n",
      "Average Training Accuracy: 0.965000\n",
      "Validation Loss: 1.518189\n",
      "Validation Accuracy: 0.950200\n",
      "Result@Batch 3400:\n",
      "Average Training Loss: 1.498579\n",
      "Average Training Accuracy: 0.973500\n",
      "Validation Loss: 1.518478\n",
      "Validation Accuracy: 0.949900\n",
      "Result@Batch 3450:\n",
      "Average Training Loss: 1.505236\n",
      "Average Training Accuracy: 0.966500\n",
      "Validation Loss: 1.516827\n",
      "Validation Accuracy: 0.953600\n",
      "Result@Batch 3500:\n",
      "Average Training Loss: 1.502389\n",
      "Average Training Accuracy: 0.968500\n",
      "Validation Loss: 1.515689\n",
      "Validation Accuracy: 0.953500\n",
      "Result@Batch 3550:\n",
      "Average Training Loss: 1.509888\n",
      "Average Training Accuracy: 0.958500\n",
      "Validation Loss: 1.516662\n",
      "Validation Accuracy: 0.951900\n",
      "Result@Batch 3600:\n",
      "Average Training Loss: 1.500046\n",
      "Average Training Accuracy: 0.970500\n",
      "Validation Loss: 1.516074\n",
      "Validation Accuracy: 0.953200\n",
      "Result@Batch 3650:\n",
      "Average Training Loss: 1.497514\n",
      "Average Training Accuracy: 0.973000\n",
      "Validation Loss: 1.515464\n",
      "Validation Accuracy: 0.954100\n",
      "Result@Batch 3700:\n",
      "Average Training Loss: 1.506266\n",
      "Average Training Accuracy: 0.964000\n",
      "Validation Loss: 1.516382\n",
      "Validation Accuracy: 0.953000\n",
      "Result@Batch 3750:\n",
      "Average Training Loss: 1.505801\n",
      "Average Training Accuracy: 0.964500\n",
      "Validation Loss: 1.516006\n",
      "Validation Accuracy: 0.954200\n",
      "Result@Batch 3800:\n",
      "Average Training Loss: 1.497345\n",
      "Average Training Accuracy: 0.974500\n",
      "Validation Loss: 1.516094\n",
      "Validation Accuracy: 0.952600\n",
      "Result@Batch 3850:\n",
      "Average Training Loss: 1.500250\n",
      "Average Training Accuracy: 0.971000\n",
      "Validation Loss: 1.515535\n",
      "Validation Accuracy: 0.953600\n",
      "Result@Batch 3900:\n",
      "Average Training Loss: 1.499308\n",
      "Average Training Accuracy: 0.970000\n",
      "Validation Loss: 1.516141\n",
      "Validation Accuracy: 0.953200\n",
      "Result@Batch 3950:\n",
      "Average Training Loss: 1.497466\n",
      "Average Training Accuracy: 0.973000\n",
      "Validation Loss: 1.515216\n",
      "Validation Accuracy: 0.953300\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "steps_per_decay = 300\n",
    "steps_per_output = 50\n",
    "\n",
    "# Create test samples\n",
    "batch_test_imgs = test_imgs\n",
    "batch_test_labels = test_labels\n",
    "\n",
    "batch_images = np.array(batch_test_imgs)\n",
    "batch_labels = np.array(batch_test_labels)\n",
    "    \n",
    "# Finish computation graph\n",
    "batch_x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "batch_y = tf.placeholder(tf.int32, shape=(None))\n",
    "train = tf.placeholder(tf.bool)\n",
    "lr_holder = tf.placeholder(tf.float32)\n",
    "loss, correct_count = get_loss_3L(batch_x,batch_y)\n",
    "lr = 0.5\n",
    "#train_step = tf.train.GradientDescentOptimizer(lr_holder).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Start a session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "with sess.as_default():\n",
    "    tf.initialize_all_variables().run()\n",
    "    avg_loss = 0.0\n",
    "    avg_accuracy = 0.0\n",
    "    for i in xrange(NUM_BATCHES):\n",
    "        # Generate data batch\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for j in xrange(BATCH_SIZE):\n",
    "            img,l = random.choice(zip(train_imgs,train_labels))\n",
    "            batch_images.append(img)\n",
    "            #l_a = np.zeros(10)\n",
    "            #l_a[l] = 1.0\n",
    "            #batch_labels.append(l_a)\n",
    "            batch_labels.append(l)\n",
    "        \n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        \n",
    "        # Train model\n",
    "        loss_val, correct_num, _ = sess.run(\n",
    "            [loss, correct_count, train_step],\n",
    "            feed_dict={batch_x:batch_images, batch_y:batch_labels})\n",
    "        \n",
    "        avg_loss += loss_val/50\n",
    "        avg_accuracy += correct_num/float(BATCH_SIZE*50)\n",
    "        \n",
    "        # Learning rate decay\n",
    "        if i!=0 and i%steps_per_decay == 0:\n",
    "            lr/=2.0\n",
    "            \n",
    "        if i!=0 and i%steps_per_output == 0:\n",
    "            # Testing\n",
    "            loss_val, correct_num = sess.run(\n",
    "                [loss, correct_count],\n",
    "                feed_dict={batch_x:batch_test_imgs, batch_y:batch_test_labels})\n",
    "            # Print out result\n",
    "            print \"Result@Batch %d:\"%i\n",
    "            print \"Average Training Loss: %f\"%avg_loss\n",
    "            print \"Average Training Accuracy: %f\"%avg_accuracy\n",
    "            print \"Validation Loss: %f\"%loss_val\n",
    "            print \"Validation Accuracy: %f\"%(correct_num/float(10000))\n",
    "            avg_loss = 0.0\n",
    "            avg_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
